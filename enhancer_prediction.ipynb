{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66z9BSEljTiN",
        "outputId": "bc62e0ad-8935-4c57-d5c3-7a0480cd83c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLNuZ30mmxRp",
        "outputId": "c8b90d1c-fc86-41d5-c5de-ba4734c6ad9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/pretraining_retaining/enhancer_retraining-master/enhancer_retraining-master\n"
          ]
        }
      ],

      
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRVIlboXm6uo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.initializers import glorot_normal, glorot_uniform\n",
        "from keras.regularizers import l1_l2, l1, l2\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCoN9gLdnC14"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluation(y_true, y_pred, threshold=.5):\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "    y_pred = (y_pred > threshold).astype('int')\n",
        "    eval_dict = {'acc': accuracy_score,\n",
        "                 'mcc': matthews_corrcoef,\n",
        "                 'precision': precision_score,\n",
        "                 'recall': recall_score,\n",
        "                 'f1': f1_score}\n",
        "    for i in eval_dict:\n",
        "        ei = eval_dict[i](y_true, y_pred)\n",
        "        print(\"{:16}{:.3f}\".format(i + \":\", ei))\n",
        "    print(\"{:16}{:.3f}\".format(\"auc:\", auc))\n",
        "    return\n",
        "\n",
        "def def_kw(D, K, N=None):\n",
        "    return D[K] if K in D.keys() else N\n",
        "\n",
        "print_eq = lambda : print('\\n', '=' * 36, sep = '')\n",
        "\n",
        "def random_set(iseed=123):\n",
        "    np.random.seed(iseed)\n",
        "    rn.seed(iseed)\n",
        "    tf.random.set_seed(iseed)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIf4LK4Zn6gi"
      },
      "outputs": [],
      "source": [
        "def build_model(in_shape=(1000, 4), **kw):\n",
        "\n",
        "    ### defaults\n",
        "    n_classes = 2\n",
        "    ## Convolution\n",
        "    n_filter = 64\n",
        "    conv_d = 1\n",
        "    # 1D\n",
        "    filter_length = 23\n",
        "    pool_length = 8\n",
        "    sub1 = 1\n",
        "    # 2D\n",
        "    conv_size = iter([(4, 23)])\n",
        "    pool_size = iter([(1, 8)])\n",
        "    sub2 = (1, 1)\n",
        "    ## Recurrent\n",
        "    n_units = 64\n",
        "    ## Dropout\n",
        "    idrop = iter([.5, .3, .3])\n",
        "    ## Dense\n",
        "    iden = iter([32])\n",
        "    ## Regularization\n",
        "    lambda_l1 = .00000001\n",
        "    lambda_l2 = .0001\n",
        "    loss = 'binary_crossentropy'\n",
        "    opt = 'adam'\n",
        "    # opt = 'adadelta'\n",
        "    # opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "    n_filter = def_kw(kw, 'n_filter', n_filter)\n",
        "    filter_length = def_kw(kw, 'filter_length', filter_length)\n",
        "    pool_length = def_kw(kw, 'pool_length', pool_length)\n",
        "    loss = def_kw(kw, 'loss', loss)\n",
        "    out = def_kw(kw, 'out', None)\n",
        "    opt = def_kw(kw, 'opt', opt)\n",
        "\n",
        "    ### modeling\n",
        "    print(\"Building model...\")\n",
        "    print(\"input shape:\\t\", in_shape)\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(in_shape, input_shape=in_shape))\n",
        "    if conv_d == 2:\n",
        "        model.add(Reshape((1, in_shape[0], in_shape[1])))\n",
        "        model.add(Permute((1, 3, 2)))\n",
        "        model.add(Conv2D(n_filter, next(conv_size),\n",
        "                         strides=sub2,\n",
        "                         padding='valid',\n",
        "                         activation='relu',\n",
        "                         data_format=\"channels_first\",\n",
        "                         ))\n",
        "        model.add(MaxPooling2D(pool_size=next(pool_size), padding='same',\n",
        "                               data_format=\"channels_first\",\n",
        "                               ))\n",
        "        model.add(Dropout(next(idrop)))\n",
        "    elif conv_d == 1:\n",
        "        model.add(Conv1D(n_filter,\n",
        "                         kernel_size=filter_length,\n",
        "                         strides=sub1,\n",
        "                         padding='valid',\n",
        "                         activation='relu',\n",
        "                         ))\n",
        "        model.add(MaxPooling1D(pool_size=pool_length, padding='same'))\n",
        "        model.add(Dropout(next(idrop)))\n",
        "    if n_units:\n",
        "        if conv_d==2:\n",
        "            model.add(Reshape((model.output_shape[1],\n",
        "                               model.output_shape[2] * model.output_shape[3])))\n",
        "            model.add(Permute((2, 1)))\n",
        "        model.add(Bidirectional(GRU(n_units), merge_mode='concat'))\n",
        "    else:\n",
        "        model.add(Flatten())\n",
        "    model.add(Dense(next(iden),\n",
        "                    activation=None))\n",
        "    model.add(Dropout(next(idrop)))\n",
        "    if loss == 'binary_crossentropy':\n",
        "        out = (1, 'sigmoid')\n",
        "    elif loss == 'categorical_crossentropy':\n",
        "        out = (n_classes, 'softmax')\n",
        "    elif loss == 'sparse_categorical_crossentropy':\n",
        "        out = (n_classes, 'softmax')\n",
        "    model.add(Dense(out[0], activation=out[1],\n",
        "                    kernel_regularizer=l1_l2(lambda_l1, lambda_l2)\n",
        "                    ))\n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL9UQ91TCgkE"
      },
      "outputs": [],
      "source": [
        " import numpy as np\n",
        " a1 = []\n",
        " b1 = []\n",
        " c1 = []\n",
        " print(a1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAKXt49ToLRZ"
      },
      "outputs": [],
      "source": [
        "def get_callbacks(**kw):\n",
        "    checkprog = def_kw(kw, 'checkprog', None)\n",
        "    earlystop = def_kw(kw, 'earlystop', None)\n",
        "    callbacks = []\n",
        "    if checkprog:\n",
        "        checkmodel = os.path.exists('./models/')\n",
        "        checkmodel = './models/' if checkmodel else './'\n",
        "        checkmodel += 'model_check_' + checkprog\n",
        "        checkmodel += '.h5'\n",
        "        checkpoint = ModelCheckpoint(filepath=checkmodel,\n",
        "                                     monitor='val_loss',\n",
        "                                     verbose=0,\n",
        "                                     save_best_only=True)\n",
        "        callbacks.append(checkpoint)\n",
        "    if earlystop:\n",
        "        assert earlystop > 0, \"earlystop must be an integer greater than 0\"\n",
        "        earlystopping = EarlyStopping(monitor='val_loss',\n",
        "                                      patience=earlystop,\n",
        "                                      verbose=0)\n",
        "        callbacks.append(earlystopping)\n",
        "    return callbacks\n",
        "\n",
        "def get_model(model=None, weights=None):\n",
        "    if not model:\n",
        "        model = build_model()\n",
        "    elif isinstance(model, str):\n",
        "        if model[-3:] == '.h5':\n",
        "            model = load_model(model)\n",
        "        elif model[-5] == '.':\n",
        "            with open(model) as f:\n",
        "                model = f.read()\n",
        "            model = model_from_yaml(model)\n",
        "        else:\n",
        "            model = model_from_yaml(model)\n",
        "    elif isinstance(model, tuple):\n",
        "        model = model_from_yaml(model[0], model[1])\n",
        "    elif isinstance(model, list):\n",
        "        model = model_from_yaml(model[0], model[1])\n",
        "    model = model\n",
        "    if weights: model.load_weights(weights)\n",
        "    return model\n",
        "\n",
        "def save_model(model, savename=None, string_type='yaml'):\n",
        "    if not savename and isinstance(model, str):\n",
        "        savename = model\n",
        "    model = get_model(model)\n",
        "    model.save('model_'+savename+'.h5')\n",
        "    with open(savename+'.'+string_type, 'w') as f:\n",
        "        if string_type == 'yaml': f.write(model.to_yaml())\n",
        "        elif string_type == 'json': f.write(model.to_json())\n",
        "    model.save_weights(savename+'.h5', overwrite=True)\n",
        "    return\n",
        "\n",
        "\n",
        "def joint_data(files, saving=None):\n",
        "    fi = files[0]\n",
        "    X_, y_ = load_data_1(fi)\n",
        "    for fi in files[1:]:\n",
        "        X_i, y_i = load_data_1(fi)\n",
        "        X_ = np.concatenate((X_, X_i))\n",
        "        y_ = np.concatenate((y_, y_i))\n",
        "    if saving:\n",
        "        with h5py.File(saving, 'w') as h5f:\n",
        "            h5f.create_dataset('seqs', data=X_)\n",
        "            h5f.create_dataset('labels', data=y_)\n",
        "        print(\"Saved in\", saving)\n",
        "    print(\"X_ & y_:\", X_.shape, y_.shape)\n",
        "    return (X_, y_)\n",
        "\n",
        "    _ = time.time()\n",
        "    results = model.predict(X_train, batch_size=batch_size)\n",
        "    pt = time.time() - _\n",
        "    print(\"predicting time: {0:.2f}s\".format(pt))\n",
        "    results = results[:, 0]\n",
        "    evaluation(y_train, results)\n",
        "\n",
        "def validation_1(datafile=None,\n",
        "                 dataset=None,\n",
        "                 model=None,\n",
        "                 batch_size=32):\n",
        "    model = get_model(model)\n",
        "    if dataset:\n",
        "        X, y = dataset\n",
        "    if datafile:\n",
        "        X, y = load_data_1(datafile)\n",
        "    _ = time.time()\n",
        "    results = model.predict(X, batch_size=batch_size)\n",
        "    pt = time.time() - _\n",
        "    print(\"predicting time: {0:.2f}s\".format(pt))\n",
        "    results = results[:, 0]\n",
        "    evaluation(y, results)\n",
        "    return (y, results)\n",
        "\n",
        "def load_data_3(datafile):\n",
        "    print(\"Loading data \", datafile)\n",
        "    with h5py.File(datafile, 'r') as h5f:\n",
        "        X_train = h5f['train_in'].value\n",
        "        y_train = h5f['train_out'].value\n",
        "        X_valid = h5f['valid_in'].value\n",
        "        y_valid = h5f['valid_out'].value\n",
        "        X_test = h5f['test_in'].value\n",
        "        y_test = h5f['test_out'].value\n",
        "    y_train = y_train.reshape(-1)\n",
        "    y_valid = y_valid.reshape(-1)\n",
        "    y_test = y_test.reshape(-1)\n",
        "    return (X_train, y_train,\n",
        "            X_valid, y_valid,\n",
        "            X_test, y_test)\n",
        "\n",
        "def validation_3(datafile, model=None,\n",
        "                 divided=False, fit=False,\n",
        "                 earlystop=8,\n",
        "                 checkprog=None,\n",
        "                 batch_size=32,\n",
        "                 n_epoch=1):\n",
        "    model = get_model(model)\n",
        "    (X_train, y_train,\n",
        "     X_valid, y_valid,\n",
        "     X_test, y_test) = load_data_3(datafile=datafile)\n",
        "    if fit:\n",
        "        print_eq()\n",
        "        print(\"training\")\n",
        "        _ = time.time()\n",
        "        callbacks = get_callbacks(checkprog=checkprog,\n",
        "                                  earlystop=earlystop)\n",
        "        if len(X_valid) > 0 or X_valid.shape != (1, 1):\n",
        "            model.fit(X_train, y_train,\n",
        "                      batch_size=batch_size, epochs=n_epoch,\n",
        "                      validation_data=(X_valid, y_valid),\n",
        "                      callbacks=callbacks, verbose=0)\n",
        "        else:\n",
        "            model.fit(X_train, y_train,\n",
        "                      batch_size=batch_size, epochs=n_epoch,\n",
        "                      callbacks=callbacks, verbose=0)\n",
        "        pt = time.time() - _\n",
        "        print(\"training time: {0:.2f}s\".format(pt))\n",
        "    history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=n_epoch,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "    if checkprog:\n",
        "        checkmodel = './models/model_check_' + checkprog + '.h5'\n",
        "        try:\n",
        "            model = get_model(checkmodel)\n",
        "        except:\n",
        "            pass\n",
        "    print_eq()\n",
        "    print('training set:')\n",
        "    validation_1(dataset=(X_train, y_train),\n",
        "                 model=model,\n",
        "                 batch_size=batch_size)\n",
        "    if len(X_valid) > 0 or X_valid.shape != (1,1):\n",
        "        print_eq()\n",
        "        print('valid set:')\n",
        "        validation_1(dataset=(X_valid, y_valid),\n",
        "                     model=model,\n",
        "                     batch_size=batch_size)\n",
        "    print_eq()\n",
        "    print('testing set:')\n",
        "    validation_1(dataset=(X_test, y_test),\n",
        "                 model=model,\n",
        "                 batch_size=batch_size)\n",
        "    return model\n",
        "def joint_p4653():\n",
        "    filehead = './data/enh_p4653_'\n",
        "    saving = filehead + '3sets.h5'\n",
        "    fnames = [filehead + 'train_%s.h5' % (i) for i in np.r_[0:5]]\n",
        "    X_train, y_train = joint_data(fnames)\n",
        "    X_valid, y_valid = load_data_1(filehead + 'valid.h5')\n",
        "    X_test, y_test = load_data_1(filehead + 'test.h5')\n",
        "    with h5py.File(saving, 'w') as h5f:\n",
        "        h5f.create_dataset('train_in', data=X_train)\n",
        "        h5f.create_dataset('train_out', data=y_train)\n",
        "        h5f.create_dataset('valid_in', data=X_valid)\n",
        "        h5f.create_dataset('valid_out', data=y_valid)\n",
        "        h5f.create_dataset('test_in', data=X_test)\n",
        "        h5f.create_dataset('test_out', data=y_test)\n",
        "    (X_train, y_train,\n",
        "     X_valid, y_valid,\n",
        "     X_test, y_test) = load_data_3(datafile=saving)\n",
        "    pass\n",
        "def split_p4653():\n",
        "    datafile = './data/enh_p4653_3sets.h5'\n",
        "    filehead = './data/enh_p4653_'\n",
        "    (X_train, y_train,\n",
        "     X_test, y_test) = load_data_3(datafile=datafile)\n",
        "    batch_size = 10000\n",
        "    i = 0\n",
        "    for si in np.r_[0:X_train.shape[0]:batch_size]:\n",
        "        ei = min(si + batch_size, X_train.shape[0])\n",
        "        with h5py.File(filehead + 'train_%s.h5' % (i), 'w') as h5f:\n",
        "            h5f.create_dataset('seqs', data=X_train[si:ei, ...])\n",
        "            h5f.create_dataset('labels', data=y_train[si:ei, ...])\n",
        "        i += 1\n",
        "    with h5py.File(filehead + 'valid.h5', 'w') as h5f:\n",
        "        h5f.create_dataset('seqs', data=X_valid)\n",
        "        h5f.create_dataset('labels', data=y_valid)\n",
        "    with h5py.File(filehead + '_test.h5', 'w') as h5f:\n",
        "        h5f.create_dataset('seqs', data=X_test)\n",
        "        h5f.create_dataset('labels', data=y_test)\n",
        "    pass\n",
        "\n",
        "def gen_p4653_model():\n",
        "    model = build_model((1000, 4))\n",
        "    datafile = './data/enh_p4653_3sets.h5'\n",
        "    prog = 'p4653'\n",
        "    model_saving = './models/model_' + prog + '.h5'\n",
        "    model = validation_3(datafile, model,\n",
        "                         batch_size=128,\n",
        "                         checkprog=prog,\n",
        "                         divided=True, fit=True)\n",
        "    model.save(model_saving)\n",
        "    return\n",
        "\n",
        "def validation_on_p4653():\n",
        "    datafile = './data/enh_p4653_3sets.h5'\n",
        "    model = './models/model_pretrained_0.h5'\n",
        "    print(\"evaluating model(%s) on data(%s)\" % (model, datafile))\n",
        "    model = validation_3(datafile, model,\n",
        "                         divided=True, fit=False)\n",
        "\n",
        "def retraining_on_vista():\n",
        "    datafile = './data/vista_human_3sets.h5'\n",
        "    model = './models/model_pretrained_0.h5'\n",
        "    model_saving = './models/model_vista.h5'\n",
        "    print(\"retraining model(%s) with data(%s)\" % (model, datafile))\n",
        "    model = validation_3(datafile, model,\n",
        "                         divided=True, fit=True,\n",
        "                         n_epoch=20)\n",
        "    model.save(model_saving)\n",
        "\n",
        "\n",
        "#b1 = a1.append(acc)\n",
        "#c1 = c1.append(n_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vzI2Qq3uxCJ",
        "outputId": "14168312-7cb3-4939-a535-11e374982e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating model(./models/model_pretrained_0.h5) on data(./data/enh_p4653_3sets.h5)\n",
            "Loading data  ./data/enh_p4653_3sets.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:105: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:106: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:107: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:108: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:109: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:110: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1333/1333 [==============================] - 389s 290ms/step - loss: 0.1340 - accuracy: 0.9479 - val_loss: 0.1709 - val_accuracy: 0.9353\n",
            "\n",
            "====================================\n",
            "training set:\n",
            "predicting time: 55.60s\n",
            "acc:            0.955\n",
            "mcc:            0.697\n",
            "precision:      0.907\n",
            "recall:         0.567\n",
            "f1:             0.698\n",
            "auc:            0.978\n",
            "\n",
            "====================================\n",
            "valid set:\n",
            "predicting time: 5.61s\n",
            "acc:            0.935\n",
            "mcc:            0.490\n",
            "precision:      0.711\n",
            "recall:         0.379\n",
            "f1:             0.495\n",
            "auc:            0.928\n",
            "\n",
            "====================================\n",
            "testing set:\n",
            "predicting time: 5.59s\n",
            "acc:            0.938\n",
            "mcc:            0.522\n",
            "precision:      0.699\n",
            "recall:         0.436\n",
            "f1:             0.537\n",
            "auc:            0.933\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-60ad4143e3f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_on_p4653\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# evaluating pretrained model on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#retraining_on_vista()       # retraining on testing set and save retrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    random_set()                # set random seed\n",
        "    #gen_p4653_model()           # generate pre-training model\n",
        "    #split_p4653()               # split data for uploading\n",
        "    #joint_p4653()               # joint splited data \n",
        "    validation_on_p4653()       # evaluating pretrained model on training data\n",
        "    #retraining_on_vista()       # retraining on testing set and save retrained model\n",
        "    plt.plot(model.history.history['acc'], label='accuracy')\n",
        "    #plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([0.5, 1])\n",
        "    plt.legend(loc = 'lower right')\n",
        "    #test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "    #print(test_acc)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "enhancer prediction.ipynb",
      "provenance": [],
      "mount_file_id": "1vYG-ZETJsFME_vIvUdyQQ2xEwQhHRVCI",
      "authorship_tag": "ABX9TyMrUs5upiKVFg9mxbiXXbax"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
